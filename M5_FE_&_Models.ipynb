{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from files\n",
    "calendar_df = pd.read_csv('calendar.csv')\n",
    "sales_df = pd.read_csv('sales_train_validation.csv')\n",
    "prices_df = pd.read_csv('sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of calendar_df is  (1969, 14)\n",
      "top 2 rows of calendar_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  wm_yr_wk   weekday  wday  month  year    d event_name_1  \\\n",
       "0  2011-01-29     11101  Saturday     1      1  2011  d_1          NaN   \n",
       "1  2011-01-30     11101    Sunday     2      1  2011  d_2          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0          NaN          NaN          NaN        0        0        0  \n",
       "1          NaN          NaN          NaN        0        0        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of calendar_df is ',calendar_df.shape)\n",
    "print('top 2 rows of calendar_df')\n",
    "calendar_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sales_df is  (30490, 1919)\n",
      "top 2 rows of sales_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "\n",
       "[2 rows x 1919 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of sales_df is ',sales_df.shape)\n",
    "print('top 2 rows of sales_df')\n",
    "sales_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prices_df is  (6841121, 4)\n",
      "top 2 rows of prices_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_id        item_id  wm_yr_wk  sell_price\n",
       "0     CA_1  HOBBIES_1_001     11325        9.58\n",
       "1     CA_1  HOBBIES_1_001     11326        9.58"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape of prices_df is ',prices_df.shape)\n",
    "print('top 2 rows of prices_df')\n",
    "prices_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Engineering </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Mem. usage decreased to 101.51 Mb (77.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "#here we are inserting the columns for validation set from days d_1914 to d_1941 which we need to forecast sales as nan \n",
    "for i in range(1914,1942):\n",
    "    sales_df['d_'+str(i)] = np.nan\n",
    "    sales_df['d_'+str(i)] = sales_df['d_'+str(i)].astype(np.float16)\n",
    "\n",
    "#to reduce the memory usage by changing the dtypes of columns of the dataframes\n",
    "calendar_df = reduce_mem_usage(calendar_df)\n",
    "prices_df = reduce_mem_usage(prices_df)\n",
    "sales_df = reduce_mem_usage(sales_df)\n",
    "\n",
    "#to transform the dataframe into vertical rows as each corresponds to each day sales of an item from a particular store\n",
    "sales_melt_df = pd.melt(sales_df, id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\n",
    "                       var_name='d',value_name='sales')\n",
    "\n",
    "#changing the dtype of object to category in order to reduce the size of dataframe\n",
    "for col in sales_melt_df.columns[:6]:\n",
    "    sales_melt_df[col] = sales_melt_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a single dataframe\n",
    "sales_melt_df = sales_melt_df.merge(calendar_df,  on='d', how='left')\n",
    "sales_melt_df = sales_melt_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre processing missing values of prices by transforming with mean price of that id\n",
    "sales_melt_df['sell_price'].fillna(sales_melt_df.groupby('id')['sell_price'].transform('mean'),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lag features </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:32<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "#creating lag features such that the for a product on current day it gets it's sales upto 3 months prior.\n",
    "shifting = 28 #shift period in order to account for 28 days to forecast\n",
    "for i in tqdm(range(9)): #num of weeks to shift here 8 weeks we consider\n",
    "    sales_melt_df['lag_'+str(shifting+(7*i))] = sales_melt_df.groupby('id')['sales'].shift(shifting+(7*i)).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rolling features </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [6:49:54<00:00, 4918.95s/it]  \n"
     ]
    }
   ],
   "source": [
    "#creating constant shift rolling agg features\n",
    "for i in tqdm([7,14,28,35,60]):\n",
    "    sales_melt_df['rolling_mean_'+str(i)] =  sales_melt_df.groupby(['id'])['lag_28'].transform(lambda x: x.rolling(i).mean())\n",
    "    sales_melt_df['rolling_median_'+str(i)] =  sales_melt_df.groupby(['id'])['lag_28'].transform(lambda x: x.rolling(i).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_melt_df.to_pickle(\"sales_melt_df\") #store the dataframe into disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_melt_df = pd.read_pickle(\"sales_melt_df\") #load the dataframe from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calender features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing dtype of calender features to category\n",
    "cal_cols = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\n",
    "for col in cal_cols:\n",
    "    sales_melt_df[col] = sales_melt_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_melt_df['date'] = pd.to_datetime(sales_melt_df['date'])\n",
    "#each day of the month\n",
    "sales_melt_df['day_of_month'] = sales_melt_df['date'].dt.day.astype(np.int8)\n",
    "#changing year value as 0 for 2011 and 1 for 2012 .... 5 for 2016\n",
    "sales_melt_df['year'] = (sales_melt_df['year'] - sales_melt_df['year'].min()).astype(np.int8)\n",
    "#week number of a day in a month ex: 29th in January corresponds to 5th week of January\n",
    "sales_melt_df['week_no_inmonth'] = sales_melt_df['day_of_month'].apply(lambda x: math.ceil(x/7)).astype(np.int8)\n",
    "#checking if the day is weekend or not\n",
    "sales_melt_df['is_weekend'] = (sales_melt_df['wday']<=2).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_melt_df.to_pickle(\"sales_melt_calfadd_df\") #store the final feature engineered dataframe to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation metric - WRMSSE</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/dhananjay3/wrmsse-evaluator-with-extra-features\n",
    "#here we are transforming the 30490 timeseries into 42840 time-series by grouping based on the 12 level hirearchy\n",
    "def convert_to_42840(df, cols, groupbys):\n",
    "    series_gen = {}\n",
    "    for i, grp in enumerate(groupbys):\n",
    "        #grop by corresponding group and calculating aggregate sales of each day\n",
    "        tmp = df.groupby(grp)[cols].sum()\n",
    "        #storing the aggregate sale values of each corresponding group\n",
    "        for j in range(len(tmp)):\n",
    "            series_gen[gen_series_name(tmp.index[j])] = tmp.iloc[j].values\n",
    "    return pd.DataFrame(series_gen).T #creating a dataframe of each corresponding group and aggregate sales each day i.e., transformed into 42840 sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method return the name of each group\n",
    "def gen_series_name(name):\n",
    "    if isinstance(name, str) | isinstance(name, int):\n",
    "        return str(name)\n",
    "    else:\n",
    "        return \"__\".join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are computing weights using the last 28 day sales of train data and their prices\n",
    "def compute_weights(train_df,valid_df,weight_cols,groupbys,fix_cols):\n",
    "    weights_map = {}\n",
    "    weight_df = train_df[[\"item_id\", \"store_id\"] + weight_cols]\n",
    "    weight_df = pd.melt(weight_df,id_vars=[\"item_id\", \"store_id\"],var_name='d',value_name='sales')\n",
    "    weight_df = weight_df.merge(calendar_df[['wm_yr_wk','d']], on='d', how='left')\n",
    "    weight_df = weight_df.merge(prices_df, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"])\n",
    "    #computing dollar sales \n",
    "    weight_df[\"dollar_sales\"] = weight_df[\"sales\"] * weight_df[\"sell_price\"]\n",
    "    weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\"dollar_sales\"]\n",
    "    weight_df = weight_df.loc[zip(train_df.item_id, train_df.store_id), :].reset_index(drop=True)\n",
    "    weight_df = pd.concat([train_df[fix_cols], weight_df],\n",
    "                          axis=1, sort=False)\n",
    "    #computing the weights for each group keys\n",
    "    for i,grp in enumerate(groupbys):\n",
    "        ser_weight = weight_df.groupby(grp)[weight_cols].sum().sum(axis=1)\n",
    "        ser_weight = ser_weight / ser_weight.sum()\n",
    "        for j in range(len(ser_weight)):\n",
    "            weights_map[gen_series_name(ser_weight.index[j])] = np.array([ser_weight.iloc[j]])\n",
    "    weights = pd.DataFrame(weights_map).T / len(groupbys) #creating a dataframe with weights corresponding to each group keys of 42840 hierachical time-series\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we compute the rmsse using the true values and predicted values along with train data which is being used to scale\n",
    "#train data is used to scale the squared-error as taking the consecutive difference of each day sales\n",
    "def compute_rmsse(train_df, valid_df, pred_df):\n",
    "    scale_lst = []\n",
    "    for i in range(len(train_df)):\n",
    "        val = train_df.iloc[i].values\n",
    "        # to consider the periods following the first non-zero demand observed for the series under evaluation.\n",
    "        val = val[np.argmax(val!=0):]\n",
    "        #to scale the squared-error as taking the consecutive difference of each day sales\n",
    "        scale = ((val[1:] - val[:-1]) ** 2).mean()\n",
    "        #storing the scale value corresponding to each time series\n",
    "        scale_lst.append(scale)\n",
    "    scale_arr = np.array(scale_lst)\n",
    "    #computing mean squared error\n",
    "    num = ((pred_df - valid_df)**2).mean(axis=1)\n",
    "    #scaled error i.e., root mean squared scaled error\n",
    "    rmsse = (num/scale_arr).map(np.sqrt)\n",
    "    return rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we return the final score value i.e., WRMSSE\n",
    "def custom_metric(train_df, valid_df, pred_df, weights):\n",
    "    #obtaing RMSSE by calling compute_rmsse function\n",
    "    rmsse = compute_rmsse(train_df, valid_df, pred_df)\n",
    "    #WRMSSE of each 42840 time-series is computed as product of corresponding weights and RMSSE respectively\n",
    "    ser_metric = pd.concat([weights, rmsse], axis=1, sort=False).prod(axis=1)\n",
    "    return np.sum(ser_metric) #aggregation of each WRMSSE of 42840 time-series to get the final WRMSSE score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Models</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Forecast sales as mean sales on that weekday for the past 4 weeks </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "\n",
       "[2 rows x 1947 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_eval_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = sales_eval_df.iloc[:,:-28]\n",
    "valid_df = sales_eval_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_d_cols = ['id','item_id','dept_id','cat_id','store_id','state_id','for_all']\n",
    "d_cols = [col for col in train_df.columns if col.startswith('d_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [02:30<00:00, 202.19it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_ser = []\n",
    "last_28_days = d_cols[-28:]\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    tmp_df = train_df.iloc[[i]]\n",
    "    tmp_df = tmp_df.loc[:,last_28_days].T.reset_index().rename(columns={i:'sales'})\n",
    "    avg_dict = {}\n",
    "    pred_lst = []\n",
    "    for j in range(7):\n",
    "        tmp_lst = []\n",
    "        for n in range(0+j,28,7):\n",
    "            tmp_lst.append(n)\n",
    "        avg_dict[j] = tmp_df.iloc[tmp_lst]['sales'].mean()\n",
    "    for p in range(1914,1942):\n",
    "        pred_lst.append(avg_dict[(p-1914)%7])\n",
    "    pred_ser.append(pred_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "predictions = np.asarray(pred_ser)\n",
    "if isinstance(predictions, np.ndarray):\n",
    "    predictions = pd.DataFrame(predictions, columns=valid_d_cols)\n",
    "predictions = pd.concat([valid_df[fixed_cols], predictions],axis=1,sort=False)\n",
    "pred_42840_df = convert_to_42840(predictions,valid_d_cols,groupbys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRMSSE = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE of the model forecasted sales, as the mean sales for the past 4 weeks of that dayofweek  0.7524207374036941\n"
     ]
    }
   ],
   "source": [
    "print(\"WRMSSE of the model forecasted sales, as the mean sales for the past 4 weeks of that dayofweek \",WRMSSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    0.50    1.25    0.75  ...    0.25    1.25    1.00   \n",
       "1       CA     all    0.25    0.00    0.25  ...    0.00    0.00    0.00   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    0.50    1.25    0.75    1.75    0.25    1.25    1.00  \n",
       "1    0.25    0.00    0.25    0.00    0.00    0.00    0.00  \n",
       "\n",
       "[2 rows x 35 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forecasted sales from from day 1914 till 1941\n",
    "predictions = pd.read_csv('predictions.csv')\n",
    "predictions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = predictions[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = predictions['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "naive_predictions = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_predictions.to_csv(\"naive_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Moving average model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(df,alpha):\n",
    "    ma_predictions = []\n",
    "    n = -alpha\n",
    "    f_days = 28\n",
    "    for j in tqdm(range(len(df))):\n",
    "        tmp_df = df.iloc[[j]] #check ma model and re-train seperately again\n",
    "        pred_lst = []\n",
    "        for i in range(f_days):\n",
    "            win = n+i\n",
    "            if n == win:\n",
    "                pred_lst.append(tmp_df.iloc[:,win:].T.mean().values)\n",
    "            if win > n:\n",
    "                pred_lst.append(np.mean(np.concatenate((tmp_df.iloc[:,win:].T.values,np.asarray(pred_lst[:i]).reshape(-1,1)),axis=0)).astype(np.float64))\n",
    "        pred_lst = [float(i) for i in pred_lst]\n",
    "        ma_predictions.append(pred_lst)\n",
    "    return np.asarray(ma_predictions).reshape(30490,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[non_d_cols+d_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyper parameter tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [07:31<00:00, 67.48it/s] \n",
      "100%|██████████| 30490/30490 [07:21<00:00, 69.13it/s] \n",
      "100%|██████████| 30490/30490 [07:15<00:00, 70.09it/s] \n",
      "100%|██████████| 30490/30490 [07:24<00:00, 68.58it/s] \n",
      "100%|██████████| 30490/30490 [07:30<00:00, 67.75it/s] \n"
     ]
    }
   ],
   "source": [
    "hyper_params = [28,35,42,49,56]\n",
    "WRMSSE_MA = {}\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "for alpha in hyper_params:\n",
    "    ma_predictions = moving_average(train_df,alpha)\n",
    "    if isinstance(ma_predictions, np.ndarray):\n",
    "        ma_predictions = pd.DataFrame(ma_predictions, columns=valid_d_cols)\n",
    "    ma_predictions = pd.concat([valid_df[fixed_cols], ma_predictions],axis=1,sort=False)\n",
    "    ma_predictions.to_csv('ma_predictions_'+str(alpha)+'.csv',index=False)\n",
    "    pred_42840_df = convert_to_42840(ma_predictions,valid_d_cols,groupbys)\n",
    "    WRMSSE = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)\n",
    "    WRMSSE_MA[alpha] = WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameter of Moving average model with lower WRMSEE is:  (35, 1.0567005821750302)\n"
     ]
    }
   ],
   "source": [
    "best_WRMSSE_ma = min(WRMSSE_MA.items(), key=lambda x: x[1])\n",
    "print('The best hyper parameter of Moving average model with lower WRMSEE is: ', best_WRMSSE_ma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    1.00    1.00    0.97  ...    0.98    1.01    1.01   \n",
       "1       CA     all    0.17    0.18    0.18  ...    0.08    0.09    0.09   \n",
       "2       CA     all    0.51    0.53    0.54  ...    0.71    0.73    0.75   \n",
       "3       CA     all    2.17    2.12    2.15  ...    2.21    2.22    2.14   \n",
       "4       CA     all    1.26    1.24    1.19  ...    1.28    1.31    1.32   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    0.98    1.01    1.04    1.07    1.07    1.07    1.02  \n",
       "1    0.09    0.09    0.10    0.10    0.10    0.11    0.11  \n",
       "2    0.77    0.80    0.82    0.81    0.78    0.75    0.74  \n",
       "3    2.08    2.09    2.15    2.12    2.18    2.21    2.28  \n",
       "4    1.33    1.31    1.35    1.36    1.37    1.35    1.36  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_alpha = best_WRMSSE_ma[0]\n",
    "ma_predictions_best = pd.read_csv('ma_predictions_'+str(best_alpha)+'.csv')\n",
    "ma_predictions_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = ma_predictions_best[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = ma_predictions_best['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "moving_average_predictions = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_predictions.to_csv(\"moving_average_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ML Models </h2>\n",
    "<h3>Random Forest</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_melt_df = pd.read_pickle(\"sales_melt_calfadd_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_median_14</th>\n",
       "      <th>rolling_mean_28</th>\n",
       "      <th>rolling_median_28</th>\n",
       "      <th>rolling_mean_35</th>\n",
       "      <th>rolling_median_35</th>\n",
       "      <th>rolling_mean_60</th>\n",
       "      <th>rolling_median_60</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>week_no_inmonth</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id    d  sales       date  wm_yr_wk  ... rolling_median_14  \\\n",
       "0       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "1       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "2       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "3       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "4       CA  d_1   0.00 2011-01-29     11101  ...               nan   \n",
       "\n",
       "   rolling_mean_28  rolling_median_28  rolling_mean_35 rolling_median_35  \\\n",
       "0              nan                nan              nan               nan   \n",
       "1              nan                nan              nan               nan   \n",
       "2              nan                nan              nan               nan   \n",
       "3              nan                nan              nan               nan   \n",
       "4              nan                nan              nan               nan   \n",
       "\n",
       "  rolling_mean_60 rolling_median_60 day_of_month week_no_inmonth is_weekend  \n",
       "0             nan               nan           29               5          1  \n",
       "1             nan               nan           29               5          1  \n",
       "2             nan               nan           29               5          1  \n",
       "3             nan               nan           29               5          1  \n",
       "4             nan               nan           29               5          1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_melt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the dtype to category for these columns in order to process the columns with label encoding\n",
    "cat_cols = ['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA','snap_TX','snap_WI']\n",
    "for col in cat_cols:\n",
    "    sales_melt_df[col] = sales_melt_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method which return the label encoded columns\n",
    "def label_encoding(df,cols):\n",
    "    for col in cols:\n",
    "        lenc = LabelEncoder()\n",
    "        df[col] = lenc.fit_transform(df[col].astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = label_encoding(sales_melt_df,cat_cols) #transforming the categorical columns to label encoded columns\n",
    "df_enc['d'] = df_enc['d'].apply(lambda x: x.split('_')[1]).astype(np.int16) #splitting the values of 'd' comlumn to take only the day number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataframe after pre-processing and feature engineering we are taking last 2 years data to train the ML model\n",
    "df_final = df_enc.loc[pd.to_datetime(df_enc['date'].dt.date) >= '2014-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train-CV-Test split</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = 1885 #end of training\n",
    "X_tr = df_final[df_final['d'] <= train_end].drop(['sales','date','weekday','wm_yr_wk'],axis=1)\n",
    "y_tr = df_final[df_final['d'] <= train_end]['sales']\n",
    "\n",
    "X_val = df_final[(df_final['d'] > train_end) & (df_final['d'] <= train_end+28)].drop(['sales','date','weekday','wm_yr_wk'],axis=1)\n",
    "y_val = df_final[(df_final['d'] > train_end) & (df_final['d'] <= train_end+28)]['sales']\n",
    "\n",
    "X_te = df_final[(df_final['d'] > train_end+28)].drop(['sales','date','weekday','wm_yr_wk'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24910330, 40), (853720, 40), (853720, 40))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_val.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the train,cv, test dataframes\n",
    "X_tr.to_pickle('X_tr')\n",
    "y_tr.to_pickle('y_tr')\n",
    "X_val.to_pickle('X_val')\n",
    "y_val.to_pickle('y_val')\n",
    "X_te.to_pickle('X_te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train,cv, test dataframes\n",
    "X_tr = pd.read_pickle(\"X_tr\")\n",
    "y_tr = pd.read_pickle(\"y_tr\")\n",
    "X_val = pd.read_pickle(\"X_val\")\n",
    "y_val = pd.read_pickle(\"y_val\")\n",
    "X_te = pd.read_pickle(\"X_te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [6:40:27<00:00, 6006.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "#group-bys to group into 12-level hierarchical time series to calculate WRMSSE \n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "alpha_lst = [50,75,100,125] #hyper-params n_estimators\n",
    "depth = 10 #fixing the depth hyper-param\n",
    "train_df = pd.concat([sales_df.loc[:,:'state_id'],sales_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "#weights corresponding all the 12-level hierarchical transformed 42840 time-series\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "#train data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "#validation data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#RandomForestRegressor model training and hyperparameter tuning\n",
    "WRMSSE_rf = {}\n",
    "for alpha in tqdm(alpha_lst):\n",
    "    rf_pred = pd.DataFrame()\n",
    "    rf_reg = RandomForestRegressor(n_estimators=alpha, max_depth=depth,n_jobs=-1)\n",
    "    rf_reg.fit(X_tr,y_tr)\n",
    "    #predicting the sales of each day from day 1886 to day 1913 cv data\n",
    "    for i in range(1886,1914):\n",
    "        rf_pred['d_'+str(i)] = rf_reg.predict(X_val[X_val['d']==i])\n",
    "    rf_pred = pd.concat([valid_df[fixed_cols], rf_pred],axis=1,sort=False)\n",
    "    #prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "    pred_42840_df = convert_to_42840(rf_pred,valid_d_cols,groupbys)\n",
    "    #Computed WRMSSE for each predictions based on hyper-parameters\n",
    "    WRMSSE_rf[(alpha,depth)] = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE for different number of base learners with depth=10 is:\n",
      " {(50, 10): 0.6728001987713877, (75, 10): 0.6741504812502015, (100, 10): 0.6731423777364821, (125, 10): 0.6736190730117694}\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE for different number of base learners with depth=10 is:\\n',WRMSSE_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using Random Forest model with depth=10 and n_estimators=50 we observe the lowest WRMSSE and hence we train the best model with these hyper parameters and forecast the sales for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(WRMSSE_rf, open('WRMSSE_rf', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRMSSE_rf = pickle.load(open('WRMSSE_rf', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameters of randomforestregressor model with lower WRMSEE is:  ((50, 10), 0.6728001987713877)\n"
     ]
    }
   ],
   "source": [
    "print('The best hyper parameters of randomforestregressor model with lower WRMSEE is: ', min(WRMSSE_rf.items(), key=lambda x: x[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with best hyperparameters to predict sales for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = 50\n",
    "best_d = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv') #reading eval data since we have true labels for days from 1914 till 1941\n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "final_predictions_rf = pd.DataFrame()\n",
    "#training the best model with the best hyper-parameters\n",
    "rf_reg_best = RandomForestRegressor(n_estimators=best_alpha, max_depth=best_d,n_jobs=-1)\n",
    "rf_reg_best.fit(X_tr,y_tr)\n",
    "#forecasting sales for the test data from day 1914 to 1941\n",
    "for i in range(1914,1942):\n",
    "    final_predictions_rf['d_'+str(i)] = rf_reg_best.predict(X_te[X_te['d']==i])\n",
    "final_predictions_rf = pd.concat([valid_df[fixed_cols], final_predictions_rf],axis=1,sort=False)\n",
    "pred_42840_df = convert_to_42840(final_predictions_rf,valid_d_cols,groupbys)\n",
    "#WRMSSE of test data\n",
    "WRMSSE_rf_te = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE on test data using randomforestregressor is:  0.7487288152251653\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE on test data using randomforestregressor is: ',WRMSSE_rf_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_reg_best.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_reg_best,'rf_reg_best.pkl') #storing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_best = joblib.load('rf_reg_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_rf.to_csv('final_predictions_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    0.89    0.89    0.89  ...    0.87    1.22    1.21   \n",
       "1       CA     all    0.25    0.25    0.25  ...    0.20    0.23    0.23   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    0.93    0.91    0.89    0.97    0.90    1.20    1.18  \n",
       "1    0.19    0.19    0.19    0.19    0.19    0.23    0.23  \n",
       "\n",
       "[2 rows x 35 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forecasted sales from from day 1914 till 1941\n",
    "final_predictions_rf = pd.read_csv('final_predictions_rf.csv')\n",
    "final_predictions_rf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = final_predictions_rf[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = final_predictions_rf['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "random_forest_predictions = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_predictions.to_csv(\"random_forest_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LightGbm Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [02:13<04:26, 133.26s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [04:12<02:09, 129.07s/it]\u001b[A\n",
      "100%|██████████| 3/3 [06:05<00:00, 121.76s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [06:05<18:15, 365.28s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [02:18<04:36, 138.07s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [04:19<02:13, 133.09s/it]\u001b[A\n",
      "100%|██████████| 3/3 [06:17<00:00, 125.76s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [12:22<12:17, 368.88s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [02:17<04:34, 137.24s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [04:22<02:13, 133.57s/it]\u001b[A\n",
      "100%|██████████| 3/3 [06:23<00:00, 127.98s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [18:46<06:13, 373.40s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [02:15<04:31, 135.99s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [04:21<02:12, 132.86s/it]\u001b[A\n",
      "100%|██████████| 3/3 [06:34<00:00, 131.49s/it]\u001b[A\n",
      "100%|██████████| 4/4 [25:20<00:00, 380.24s/it]\n"
     ]
    }
   ],
   "source": [
    "#group-bys to group into 12-level hierarchical time series to calculate WRMSSE \n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "alpha_lst = [50,75,100,125] #hyper-params num_leaves\n",
    "lr_lst = [0.025,0.05,0.075] #hyper-params learning_rate\n",
    "train_df = pd.concat([sales_df.loc[:,:'state_id'],sales_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "#weights corresponding all the 12-level hierarchical transformed 42840 time-series\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "#train data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "#validation data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#LGBMRegressor model training and hyperparameter tuning\n",
    "WRMSSE_lgb = {}\n",
    "for alpha in tqdm(alpha_lst):\n",
    "    for lr in tqdm(lr_lst):\n",
    "        lgb_pred = pd.DataFrame()\n",
    "        lgb_reg = lgb.LGBMRegressor(num_leaves=alpha,n_estimators=100,learning_rate=lr,n_jobs=-1)\n",
    "        lgb_reg.fit(X_tr,y_tr) #fit the model\n",
    "        #predicting the sales of each day from day 1886 to day 1913 cv data\n",
    "        for i in range(1886,1914):\n",
    "            lgb_pred['d_'+str(i)] = lgb_reg.predict(X_val[X_val['d']==i])\n",
    "        lgb_pred = pd.concat([valid_df[fixed_cols], lgb_pred],axis=1,sort=False)\n",
    "        #prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "        pred_42840_df = convert_to_42840(lgb_pred,valid_d_cols,groupbys)\n",
    "        #Computed WRMSSE for each predictions based on hyper-parameters\n",
    "        WRMSSE_lgb[(alpha,lr)] = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(WRMSSE_lgb, open('WRMSSE_lgb', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE for different num_leaves and learning_rate for lgbmregressor model is:\n",
      " {(50, 0.025): 0.8104295087418298, (50, 0.05): 0.6488636984074175, (50, 0.075): 0.5943479203889876, (75, 0.025): 0.7846888510211083, (75, 0.05): 0.623858513434854, (75, 0.075): 0.5920263861529307, (100, 0.025): 0.767089298738303, (100, 0.05): 0.6108440595919674, (100, 0.075): 0.5881100575208845, (125, 0.025): 0.7594463094565398, (125, 0.05): 0.6162678647824176, (125, 0.075): 0.5880677789144512}\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE for different num_leaves and learning_rate for lgbmregressor model is:\\n',WRMSSE_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameters of lgbmregressor model with lower WRMSEE is:  ((125, 0.075), 0.5880677789144512)\n"
     ]
    }
   ],
   "source": [
    "print('The best hyper parameters of lgbmregressor model with lower WRMSEE is: ', min(WRMSSE_lgb.items(), key=lambda x: x[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with best hyperparameters to predict sales for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv') #reading eval data since we have true labels for days from 1914 till 1941\n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "alpha = 125\n",
    "lr = 0.075\n",
    "final_predictions_lgb = pd.DataFrame()\n",
    "#training the best model with the best hyper-parameters\n",
    "lgb_reg_best = lgb.LGBMRegressor(num_leaves=alpha,n_estimators=100,learning_rate=lr,n_jobs=-1)\n",
    "lgb_reg_best.fit(X_tr,y_tr) #fit the model\n",
    "#forecasting sales for the test data from day 1914 to 1941 test data\n",
    "for i in range(1914,1942):\n",
    "    final_predictions_lgb['d_'+str(i)] = lgb_reg_best.predict(X_te[X_te['d']==i])\n",
    "final_predictions_lgb = pd.concat([valid_df[fixed_cols], final_predictions_lgb],axis=1,sort=False)\n",
    "pred_42840_df = convert_to_42840(final_predictions_lgb,valid_d_cols,groupbys)\n",
    "#WRMSSE of test data\n",
    "WRMSSE_lgb_te = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE on test data using lgbmregressor is:  0.6087447985982151\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE on test data using lgbmregressor is: ',WRMSSE_lgb_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgb_reg_best.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgb_reg_best,'lgb_reg_best.pkl') #store the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg_best = joblib.load('lgb_reg_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_lgb.to_csv('final_predictions_lgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.67</td>\n",
       "      <td>...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    0.95    0.85    0.84  ...    0.90    1.36    1.25   \n",
       "1       CA     all    0.35    0.29    0.29  ...    0.20    0.28    0.27   \n",
       "2       CA     all    0.39    0.35    0.35  ...    0.50    0.72    0.78   \n",
       "3       CA     all    1.88    1.68    1.67  ...    1.59    2.04    2.29   \n",
       "4       CA     all    0.97    0.94    1.05  ...    1.05    1.29    1.30   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    1.00    0.91    0.85    0.95    0.88    1.27    1.22  \n",
       "1    0.20    0.17    0.21    0.17    0.20    0.27    0.27  \n",
       "2    0.66    0.65    0.64    0.54    0.63    0.84    0.84  \n",
       "3    1.76    1.59    1.48    1.44    1.70    2.66    2.56  \n",
       "4    0.92    0.90    0.91    0.95    1.08    1.41    1.60  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forecasted sales from from day 1914 till 1941 test data\n",
    "final_predictions_lgb = pd.read_csv('final_predictions_lgb.csv').drop(['Unnamed: 0'],axis=1)\n",
    "final_predictions_lgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = final_predictions_lgb[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = final_predictions_lgb['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "submit_lgb = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_lgb.to_csv(\"submit_lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Linear model - SGDRegressor with squared loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X_tr_scaled = scalar.fit_transform(X_tr)\n",
    "X_val_scaled = scalar.fit_transform(X_val)\n",
    "X_te_scaled = scalar.fit_transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [16:32<00:00, 141.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#group-bys to group into 12-level hierarchical time series to calculate WRMSSE \n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "alpha_lst = [0.0001,0.001,0.01,0.1,0.5,0.7,1] #hyper-param constant that multiplies the regularization term\n",
    "train_df = pd.concat([sales_df.loc[:,:'state_id'],sales_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "#weights corresponding all the 12-level hierarchical transformed 42840 time-series\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "#train data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "#validation data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#SGDRegressor model training and hyperparameter tuning\n",
    "WRMSSE_sgd = {}\n",
    "for alpha in tqdm(alpha_lst):\n",
    "    sgdreg = SGDRegressor(alpha=alpha,loss='squared_loss',early_stopping=True)\n",
    "    sgdreg.fit(X_tr_scaled,y_tr.values)\n",
    "    sgd_predictions = sgdreg.predict(X_val_scaled)\n",
    "    \n",
    "    #slicing the predictions such that to get each day predictions of all the products of cv data\n",
    "    start = 0\n",
    "    t = int(X_val.iloc[0]['d'])\n",
    "    sgd_pred_df = pd.DataFrame()\n",
    "    while start < len(sgd_predictions):\n",
    "        end = start + 30490\n",
    "        sgd_pred_df['d_'+str(t)] = sgd_predictions[start:end]\n",
    "        start = end\n",
    "        t = t+1\n",
    "    sgd_pred_df = pd.concat([valid_df[fixed_cols], sgd_pred_df],axis=1,sort=False)\n",
    "    #prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "    pred_42840_df = convert_to_42840(sgd_pred_df,valid_d_cols,groupbys)\n",
    "    #Computed WRMSSE for each predictions based on hyper-parameters\n",
    "    WRMSSE_sgd[alpha] = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE for different learning_rate of SGDRegressor model is:\n",
      " {0.0001: 1.0796299168842254, 0.001: 0.9225506211122607, 0.01: 0.9016064697738859, 0.1: 0.8931220899703974, 0.5: 0.9762900342384021, 0.7: 0.9710781455217893, 1: 0.949832124699446}\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE for different learning_rate of SGDRegressor model is:\\n',WRMSSE_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(WRMSSE_sgd, open('WRMSSE_sgd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameters of SGDRegressor model with lower WRMSEE is:  (0.1, 0.8931220899703974)\n"
     ]
    }
   ],
   "source": [
    "print('The best hyper parameters of SGDRegressor model with lower WRMSEE is: ', min(WRMSSE_sgd.items(), key=lambda x: x[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training with best hyperparameters to predict sales for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv') #reading eval data since we have true labels for days from 1914 till 1941\n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#training the best model with the best hyper-parameters\n",
    "best_alpha = 0.1\n",
    "sgdreg = SGDRegressor(alpha=best_alpha,loss='squared_loss',early_stopping=True)\n",
    "sgdreg.fit(X_tr_scaled,y_tr.values)\n",
    "sgd_predictions = sgdreg.predict(X_te_scaled)\n",
    "\n",
    "#slicing the predictions such that to get each day predictions of all the products of test data\n",
    "start = 0\n",
    "t = int(X_te.iloc[0]['d'])\n",
    "sgd_pred_df = pd.DataFrame()\n",
    "while start < len(sgd_predictions):\n",
    "    end = start + 30490\n",
    "    sgd_pred_df['d_'+str(t)] = sgd_predictions[start:end]\n",
    "    start = end\n",
    "    t = t+1\n",
    "sgd_pred_df = pd.concat([valid_df[fixed_cols], sgd_pred_df],axis=1,sort=False)\n",
    "#prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "pred_42840_df = convert_to_42840(sgd_pred_df,valid_d_cols,groupbys)\n",
    "#Computed WRMSSE for each predictions based on hyper-parameters\n",
    "WRMSSE_sgd_te = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE on test data using SGDRegressor is:  0.9682722099828945\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE on test data using SGDRegressor is: ',WRMSSE_sgd_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_sgdreg.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sgdreg,'best_sgdreg.pkl') #store the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ada_reg = joblib.load('best_ada_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pred_df.to_csv('final_predictions_sgd.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.27</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    0.83    0.67    0.62  ...    0.98    1.23    0.93   \n",
       "1       CA     all    0.41    0.21    0.18  ...    0.07    0.39    0.33   \n",
       "2       CA     all    0.18    0.26    0.13  ...    0.50    0.83    0.87   \n",
       "3       CA     all    1.88    1.46    1.27  ...    1.44    1.99    2.60   \n",
       "4       CA     all    0.84    0.86    1.00  ...    1.04    1.09    1.37   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    0.89    0.87    0.83    0.91    0.78    1.07    1.19  \n",
       "1    0.07   -0.04    0.05    0.12    0.01    0.26    0.23  \n",
       "2    0.45    0.52    0.48    0.40    0.52    0.81    0.75  \n",
       "3    1.81    1.25    1.14    1.48    1.81    2.69    2.51  \n",
       "4    0.82    0.86    1.00    1.18    0.94    1.23    1.75  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forecasted sales from from day 1914 till 1941 test data\n",
    "final_predictions_sgd = pd.read_csv('final_predictions_sgd.csv')\n",
    "final_predictions_sgd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = final_predictions_sgd[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = final_predictions_sgd['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "submit_sgd = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_sgd.to_csv(\"submit_sgd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> AdaBoost model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [8:57:21<00:00, 10747.14s/it]  \n"
     ]
    }
   ],
   "source": [
    "#group-bys to group into 12-level hierarchical time series to calculate WRMSSE \n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = pd.concat([sales_df.loc[:,:'state_id'],sales_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "#weights corresponding all the 12-level hierarchical transformed 42840 time-series\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "#train data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "#validation data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#AdaBoostRegressor model training and hyperparameter tuning\n",
    "estimators = 50 #fixing the n_estimators hyper-param\n",
    "lr_lst = [0.025,0.05,0.075] #hyper-params learning_rate\n",
    "WRMSSE_ada = {}\n",
    "for lr in tqdm(lr_lst):\n",
    "    ada_pred = pd.DataFrame()\n",
    "    dt_rg = DecisionTreeRegressor(max_depth=10,max_features=10,random_state=42)\n",
    "    ada_reg = AdaBoostRegressor(base_estimator=dt_rg,n_estimators=estimators,learning_rate=lr,loss='square')\n",
    "    ada_reg.fit(X_tr,y_tr)\n",
    "    #predicting the sales of each day from day 1886 to day 1913 cv data\n",
    "    for i in range(1886,1914):\n",
    "        ada_pred['d_'+str(i)] = ada_reg.predict(X_val[X_val['d']==i])\n",
    "    ada_pred = pd.concat([valid_df[fixed_cols], ada_pred],axis=1,sort=False)\n",
    "    #prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "    pred_42840_df = convert_to_42840(ada_pred,valid_d_cols,groupbys)\n",
    "    #Computed WRMSSE for each predictions based on hyper-parameters\n",
    "    WRMSSE_ada[lr] = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE for different learning_rate of AdaBoost model is:\n",
      " {0.025: 0.7052531906511087, 0.05: 0.7045868694884032, 0.075: 0.7087491212928341}\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE for different learning_rate of AdaBoost model is:\\n',WRMSSE_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(WRMSSE_ada, open('WRMSSE_ada', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameters of AdaBoost model with lower WRMSEE is:  (0.05, 0.7045868694884032)\n"
     ]
    }
   ],
   "source": [
    "print('The best hyper parameters of AdaBoost model with lower WRMSEE is: ', min(WRMSSE_ada.items(), key=lambda x: x[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training with best hyperparameters to predict sales for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv') #reading eval data since we have true labels for days from 1914 till 1941\n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "estimators = 50 \n",
    "best_lr = 0.05\n",
    "final_predictions_ada = pd.DataFrame()\n",
    "#training the best model with the best hyper-parameters\n",
    "dt_rg = DecisionTreeRegressor(max_depth=10,max_features=10,random_state=42)\n",
    "best_ada_reg = AdaBoostRegressor(base_estimator=dt_rg,n_estimators=estimators,learning_rate=best_lr,loss='square')\n",
    "best_ada_reg.fit(X_tr,y_tr)\n",
    "#predicting the sales of each day from day 1914 to day 1941 test data\n",
    "for i in range(1914,1942):\n",
    "    final_predictions_ada['d_'+str(i)] = best_ada_reg.predict(X_te[X_te['d']==i])\n",
    "final_predictions_ada = pd.concat([valid_df[fixed_cols], final_predictions_ada],axis=1,sort=False)\n",
    "#prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "pred_42840_df = convert_to_42840(final_predictions_ada,valid_d_cols,groupbys)\n",
    "#Computed WRMSSE for each predictions based on hyper-parameters\n",
    "WRMSSE_ada_te = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE on test data using AdaBoostRegressor is:  0.7944137374584284\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE on test data using AdaBoostRegressor is: ',WRMSSE_ada_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_ada_reg.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_ada_reg,'best_ada_reg.pkl') #store the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ada_reg = joblib.load('best_ada_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_ada.to_csv('final_predictions_ada.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.74</td>\n",
       "      <td>...</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    0.92    0.89    0.89  ...    0.87    1.21    1.20   \n",
       "1       CA     all    0.32    0.32    0.32  ...    0.20    0.19    0.20   \n",
       "2       CA     all    0.42    0.42    0.42  ...    0.38    0.47    0.55   \n",
       "3       CA     all    1.88    1.82    1.74  ...    1.65    2.09    2.30   \n",
       "4       CA     all    0.98    0.99    1.01  ...    1.06    1.32    1.32   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    0.97    0.95    0.93    0.96    0.89    1.19    1.17  \n",
       "1    0.19    0.18    0.21    0.20    0.18    0.18    0.17  \n",
       "2    0.51    0.53    0.60    0.53    0.53    0.68    0.68  \n",
       "3    1.72    1.65    1.60    1.59    1.61    2.38    2.29  \n",
       "4    0.99    1.00    0.98    1.02    1.04    1.34    1.43  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forecasted sales from from day 1914 till 1941 test data\n",
    "final_predictions_ada = pd.read_csv('final_predictions_ada.csv')\n",
    "final_predictions_ada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = final_predictions_ada[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = final_predictions_ada['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "submit_ada = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_ada.to_csv(\"submit_ada.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Custom ensemble model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train,cv, test dataframes\n",
    "X_tr = pd.read_pickle(\"X_tr\")\n",
    "y_tr = pd.read_pickle(\"y_tr\")\n",
    "X_val = pd.read_pickle(\"X_val\")\n",
    "y_val = pd.read_pickle(\"y_val\")\n",
    "X_te = pd.read_pickle(\"X_te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping initial 3 days rows from 1069-1071 in order for the time-split of data 80-20 to have integer number of days\n",
    "drop_d = [1069,1070,1071]\n",
    "X_tr = X_tr[~X_tr['d'].isin(drop_d)]\n",
    "y_tr = y_tr.loc[X_tr.index]\n",
    "#getting labels of days from 1914-1941 and use them for evaluating WRMSSE score\n",
    "df1 = pd.read_csv('sales_train_evaluation.csv')\n",
    "df1 = pd.melt(df1.iloc[:,-28:],var_name='d',value_name='sales')\n",
    "y_te = df1['sales']\n",
    "X_tr.reset_index(drop=True,inplace=True)\n",
    "X_val.reset_index(drop=True,inplace=True)\n",
    "X_te.reset_index(drop=True,inplace=True)\n",
    "y_tr.reset_index(drop=True,inplace=True)\n",
    "y_val.reset_index(drop=True,inplace=True)\n",
    "y_te.reset_index(drop=True,inplace=True)\n",
    "X = pd.concat([X_tr,X_val,X_te],sort=False)\n",
    "X.reset_index(drop=True,inplace=True)\n",
    "y = pd.concat([y_tr,y_val,y_te],sort=False)\n",
    "y.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method returns the dataframes splitted based on specified percentage\n",
    "def split_data(X,y,per):\n",
    "    n_rows = int(len(X)*per)\n",
    "    X1 = X.head(n_rows)\n",
    "    y1 = y.head(n_rows)\n",
    "    X2 = X.loc[X.index.difference(X1.index, sort=False)]\n",
    "    y2 = y.loc[y.index.difference(y1.index, sort=False)]\n",
    "    return X1,y1,X2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formulating X_train,X_test & D1,D2 along with their target values\n",
    "X_train,y_train,X_test,y_test = split_data(X,y,0.8)\n",
    "D1_x,D1_y,D2_x,D2_y = split_data(X_train,y_train,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a custom method where we return the predictions of the test data X_test.\n",
    "#we train N base models with sampled with replacement D1 data and using those models \n",
    "#we get the predictions as features from each model for D2 data and train the meta model \n",
    "#with predicted values of D2 based on N models, D2_y\n",
    "#getting predictions of X_test from N base models we get the final predictions from trained meta model.\n",
    "def custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,N):\n",
    "    #defing some of variables to be used dynamically for base models, sample dataframes, features, predictions\n",
    "    sample_D1_x = ['d'+str(i)+'_x' for i in range(1,N+1)]\n",
    "    sample_D1_y = ['d'+str(i)+'_y' for i in range(1,N+1)]\n",
    "    base_models = ['M'+str(i) for i in range(1,N+1)]\n",
    "    preds_D2_x  = ['pred_d2_'+str(i) for i in range(int(D2_x.iloc[0]['d']),int(D2_x.iloc[-1]['d'])+1)]\n",
    "    preds_X_test  = ['pred_X_test_'+str(i) for i in range(int(X_test.iloc[0]['d']),int(X_test.iloc[-1]['d'])+1)]\n",
    "    features_D2_pred = ['D2_f_'+str(i) for i in range(1,N+1)]\n",
    "    features_X_test_pred = ['X_test_f_'+str(i) for i in range(1,N+1)]\n",
    "    D2_pred = pd.DataFrame()\n",
    "    X_test_pred = pd.DataFrame()\n",
    "    #N represents number of base models\n",
    "    for i in tqdm(range(N)):\n",
    "        #getting the sampled with replacement D1_x\n",
    "        sample_D1_x[i] = D1_x.sample(frac=1,replace=True)\n",
    "        #getting the sampled with replacement D1_y\n",
    "        sample_D1_y[i] = D1_y.loc[sample_D1_x[i].index]\n",
    "        #defing each base model\n",
    "        base_models[i] = DecisionTreeRegressor(max_depth=10,max_features=10,random_state=42)\n",
    "        #training each base model\n",
    "        base_models[i].fit(sample_D1_x[i],sample_D1_y[i])\n",
    "        #predicting for all the days of D2_x using traing N base models and using them as features\n",
    "        preds_D2_x[i] = pd.DataFrame()\n",
    "        for j in range(int(D2_x.iloc[0]['d']),int(D2_x.iloc[-1]['d'])+1):\n",
    "            preds_D2_x[i]['d_'+str(j)] = base_models[i].predict(D2_x[D2_x['d']==j])\n",
    "        df1 = pd.melt(preds_D2_x[i],var_name='d',value_name='sales')\n",
    "        #creating dataframe with features as predictions of D2_x obtained from trained N base models\n",
    "        D2_pred[features_D2_pred[i]] = df1['sales'].values\n",
    "        #predicting for all the days of X_test using traing N base models and using them as features\n",
    "        preds_X_test[i] = pd.DataFrame()\n",
    "        for k in range(int(X_test.iloc[0]['d']),int(X_test.iloc[-1]['d'])+1):\n",
    "            preds_X_test[i]['d_'+str(k)] = base_models[i].predict(X_test[X_test['d']==k])\n",
    "        df2 = pd.melt(preds_X_test[i],var_name='d',value_name='sales')\n",
    "        #creating dataframe with features as predictions of X_test obtained from trained N base models\n",
    "        X_test_pred[features_X_test_pred[i]] = df2['sales'].values\n",
    "    #training meta-model with D2_pred,D2_y\n",
    "    meta_model = xgb.XGBRegressor(n_estimators=200,learning_rate=0.05,max_depth=10,n_jobs=-1,\n",
    "                                  colsample_bytree=0.3,subsample=1,random_state=42)\n",
    "    #fit the model\n",
    "    meta_model.fit(D2_pred.values,D2_y.values)\n",
    "    #getting the predictions for X_test_pred from trained meta-model\n",
    "    meta_model_preds = meta_model.predict(X_test_pred.values)\n",
    "    return meta_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:28<00:00, 68.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#calling the custom_ensemble method and returns the predictions of X_test\n",
    "custom_ensemle_predictions = custom_ensemble(D1_x,D1_y,D2_x,D2_y,X_test,y_test,10)\n",
    "#storing the predictions of X_test\n",
    "pickle.dump(custom_ensemle_predictions, open(\"custom_ensemle_predictions\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_eval_df = pd.read_csv('sales_train_evaluation.csv') #reading eval data since we have true labels for days from 1914 till 1941\n",
    "groupbys = ('for_all', 'state_id', 'store_id', 'cat_id', 'dept_id',['state_id', 'cat_id'],  \n",
    "            ['state_id', 'dept_id'], ['store_id', 'cat_id'],['store_id', 'dept_id'], 'item_id', \n",
    "            ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
    "train_df = pd.concat([sales_eval_df.loc[:,:'state_id'],sales_eval_df.loc[:,'d_1069':]],axis=1,sort=False)\n",
    "valid_df = train_df.iloc[:,-28:].copy()\n",
    "train_d_cols = [col for col in train_df.columns if col.startswith('d_')]\n",
    "weight_cols = train_df.iloc[:,-28:].columns.tolist()\n",
    "train_df['for_all'] = \"all\" #for level 1 aggregation\n",
    "fixed_cols = [col for col in train_df.columns if not col.startswith('d_')]\n",
    "valid_d_cols = [col for col in valid_df.columns if col.startswith('d_')]\n",
    "if not all([col in valid_df.columns for col in fixed_cols]):\n",
    "    valid_df = pd.concat([train_df[fixed_cols],valid_df],axis=1,sort=False)\n",
    "weight_df = compute_weights(train_df,valid_df,weight_cols,groupbys,fixed_cols)\n",
    "train_42840_df = convert_to_42840(train_df, train_d_cols, groupbys)\n",
    "valid_42840_df = convert_to_42840(valid_df, valid_d_cols, groupbys)\n",
    "\n",
    "#loading the X_test predictions\n",
    "custom_ensemle_predictions = pickle.load(open('custom_ensemle_predictions', 'rb'))\n",
    "\n",
    "#slicing the predictions such that to get each day predictions of all the products of test data\n",
    "start = 0\n",
    "t = 1768\n",
    "custom_ensemle_predictions_df = pd.DataFrame()\n",
    "while start < len(custom_ensemle_predictions):\n",
    "    end = start + 30490\n",
    "    custom_ensemle_predictions_df['d_'+str(t)] = custom_ensemle_predictions[start:end]\n",
    "    start = end\n",
    "    t = t+1\n",
    "\n",
    "forecast_horizon_pred =  custom_ensemle_predictions_df.iloc[:,-28:]\n",
    "forecast_horizon_pred = pd.concat([valid_df[fixed_cols], forecast_horizon_pred],axis=1,sort=False)\n",
    "#prediction data transformed from 30490 timeseries to 42840 hirerachichal time-series\n",
    "pred_42840_df = convert_to_42840(forecast_horizon_pred,valid_d_cols,groupbys)\n",
    "#Computed WRMSSE for each predictions based on hyper-parameters\n",
    "WRMSSE_custom_te = custom_metric(train_42840_df,valid_42840_df,pred_42840_df,weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRMSSE on test data using custom ensemble model is:  0.7988353134940696\n"
     ]
    }
   ],
   "source": [
    "print('WRMSSE on test data using custom ensemble model is: ',WRMSSE_custom_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon_pred.to_csv('forecast_horizon_pred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>for_all</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.62</td>\n",
       "      <td>...</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>all</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id for_all  d_1914  d_1915  d_1916  ...  d_1932  d_1933  d_1934  \\\n",
       "0       CA     all    0.94    0.82    0.82  ...    0.88    1.24    1.15   \n",
       "1       CA     all    0.28    0.28    0.28  ...    0.17    0.24    0.24   \n",
       "2       CA     all    0.42    0.42    0.42  ...    0.38    0.49    0.64   \n",
       "3       CA     all    1.84    1.83    1.62  ...    1.66    2.23    2.44   \n",
       "4       CA     all    0.90    0.91    1.00  ...    0.95    1.28    1.30   \n",
       "\n",
       "   d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0    0.90    0.97    0.93    1.02    0.99    1.25    1.23  \n",
       "1    0.17    0.17    0.18    0.18    0.18    0.23    0.23  \n",
       "2    0.51    0.51    0.68    0.51    0.62    0.79    0.79  \n",
       "3    1.74    1.62    1.48    1.56    1.65    2.65    2.32  \n",
       "4    0.91    0.93    0.94    0.95    0.98    1.34    1.43  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forecasted sales from from day 1914 till 1941 test data\n",
    "forecast_horizon_pred = pd.read_csv('forecast_horizon_pred.csv')\n",
    "forecast_horizon_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_val = forecast_horizon_pred[['id']]\n",
    "for i in range(28):\n",
    "    submit_val['F'+str(i+1)] = forecast_horizon_pred['d_'+str(1914+i)]\n",
    "submit_val['id'] =  submit_val['id'].apply(lambda x: x.replace('evaluation','validation'))\n",
    "submit_eval = submit_val.copy()\n",
    "submit_eval[\"id\"] = submit_eval[\"id\"].apply(lambda x: x.replace('validation','evaluation'))\n",
    "submit_custom_ensemble = submit_val.append(submit_eval).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_custom_ensemble.to_csv(\"submit_custom_ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+--------+\n",
      "|                       Model                       | WRMSSE |\n",
      "+---------------------------------------------------+--------+\n",
      "| Mean sales for the past 4 weeks of that dayofweek | 0.7524 |\n",
      "|                   Moving AVerage                  | 1.0567 |\n",
      "|               RandomForestRegressor               | 0.7487 |\n",
      "|                   LgbmRegressor                   | 0.6087 |\n",
      "|                    SGDRegressor                   | 0.9682 |\n",
      "|                 AdaBoostRegressor                 | 0.7944 |\n",
      "|      Custom_ensemble(num of base-models = 10)     | 0.7988 |\n",
      "+---------------------------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "res = PrettyTable()\n",
    "res.field_names = [\"Model\", \"WRMSSE\"]\n",
    "res.add_row([\"Mean sales for the past 4 weeks of that dayofweek\", 0.7524])\n",
    "res.add_row([\"Moving AVerage\", 1.0567])\n",
    "res.add_row([\"RandomForestRegressor\",0.7487])\n",
    "res.add_row([\"LgbmRegressor\", 0.6087])\n",
    "res.add_row([\"SGDRegressor\", 0.9682])\n",
    "res.add_row([\"AdaBoostRegressor\", 0.7944])\n",
    "res.add_row([\"Custom_ensemble(num of base-models = 10)\", 0.7988])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusions</h3>\n",
    "1. Feature engineering is performed and the features created are:\n",
    "\n",
    "    a) Lag features: 'lag_28', 'lag_35',\n",
    "       'lag_42', 'lag_49', 'lag_56', 'lag_63', 'lag_70', 'lag_77', 'lag_84'\n",
    "       ex: 'lag_28' implies sales value of 28 days prior on a given day.\n",
    "       \n",
    "    b) Rolling features:\n",
    "        i) Mean features: 'rolling_mean_7',  'rolling_mean_14',\n",
    "        'rolling_mean_28','rolling_mean_35', 'rolling_mean_60'\n",
    "        ii) Median features: 'rolling_median_7','rolling_median_14',\n",
    "        'rolling_median_28', 'rolling_median_35','rolling_median_60'\n",
    "    c) Calender features:\n",
    "        'day_of_month', 'week_no_inmonth', 'is_weekend'\n",
    "2. After performing the pre-processing and feature engineering, we split the data as time-based splitting.\n",
    "3. Forecast of sales has been carried out using some of baseline model and ML models and each of the performance is observed.\n",
    "4. It can be observed from metric score that tree based models perform better compared to liner models\n",
    "5. Of all the models LightGbm boosting model performs well as it is able to forecast with lower score(WRMSSE) = 0.6087.\n",
    "6. Using the custom-ensemble model with number of base models = 10 we get decent WRMSSE score of 0.79 which can be further improved by tuning number of base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kaggle score of the models\n",
    "<img src=\"https://imgur.com/J5IXB2l.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
